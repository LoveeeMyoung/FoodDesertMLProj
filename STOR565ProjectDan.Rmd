Libraries
```{r}
library(ggplot2) # For plotting
library(dplyr) # For data manipulation
library(tidyr) # For data tidying
library(readr) # For reading data
library(corrplot) # For correlation plots
library(GGally) # For more visualizations
```
Reading Data
```{r}
data <- read.csv("https://raw.githubusercontent.com/barrettb/STOR565Project/main/UpdatedData1127.csv")
datatrain <- read.csv("https://raw.githubusercontent.com/barrettb/STOR565Project/main/GH.train.csv")
datatest <- read.csv("https://raw.githubusercontent.com/barrettb/STOR565Project/main/GH.test.csv")
```
Histograms to show normality
```{r}
ggplot(data, aes(x = Overall.Mortality.Rate)) +
  geom_histogram(binwidth = 0.001, fill = "blue", color = "black") +
  labs(title = "Histogram of Overall Mortality Rate",
       x = "Overall Mortality Rate",
       y = "Frequency") +
  theme_minimal()

ggplot(data, aes(x = Population)) +
  geom_histogram(binwidth = 500000, fill = "blue", color = "black") +
  labs(title = "Histogram of Population",
       x = "Population",
       y = "Frequency") +
  theme_minimal()

ggplot(data, aes(x = Sq.Miles)) +
  geom_histogram(binwidth = 2000, fill = "blue", color = "black") +
  labs(title = "Histogram of Square Miles",
       x = "Square Miles",
       y = "Frequency") +
  theme_minimal()

ggplot(data, aes(x = logarithm_pop)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black") +
  labs(title = "Histogram of log(Population)",
       x = "log(Population)",
       y = "Frequency") +
  theme_minimal()

ggplot(data, aes(x = logarithm_miles)) +
  geom_histogram(binwidth = .5, fill = "blue", color = "black") +
  labs(title = "Histogram of log(Square Miles)",
       x = "log(Square Miles)",
       y = "Frequency") +
  theme_minimal()
```
Cleaning Data
```{r}
#full data
datagrocery <- cbind(data[,60:61], data[, 6:32])
datahealth <- data[,c(34,37,40,43,46,49,52,55)]
datagroceryhealth <- cbind(datagrocery,datahealth)
#train data
datatraingrocery <- cbind(datatrain[,59:60], datatrain[, 5:31])
datatrainhealth <- datatrain[,c(33,36,39,42,45,48,51,54)]
datatraingroceryhealth <- cbind(datagrocery,datahealth)
#test data
datatestgrocery <- cbind(datatest[,59:60], datatest[, 5:31])
datatesthealth <- datatest[,c(33,36,39,42,45,48,51,54)]
datatestgroceryhealth <- cbind(datagrocery,datahealth)
```

```{r}
cor_matrix <- cor(datagrocery)
corrplot(cor_matrix, method = "color", type = "upper",  
         tl.col = "black", tl.srt = 45)

cor_matrix <- cor(datahealth)
corrplot(cor_matrix, method = "color", type = "upper",  
         tl.col = "black", tl.srt = 45)

cor_matrix <- cor(datagroceryhealth)
corrplot(cor_matrix, method = "color", type = "upper",  
         tl.col = "black", tl.srt = 45)
```
Linear Regression
```{r}

# Assuming 'data' is your DataFrame
results <- list()

for (response_col in names(datatrainhealth)) {
  # Selecting the current column as response and the rest as predictors
  datatraingrocery[,30] <- datatrainhealth[[response_col]]

  # Running the linear regression
  model <- lm(V30 ~ ., data = datatraingrocery)
  
  datatraingrocery <- datatraingrocery[,-30]
  
  # Predict on the training data
  train_predictions <- predict(model, newdata = datatraingrocery)

  # Predict on the testing data
  test_predictions <- predict(model, newdata = datatestgrocery)
  
  # Calculate RMSE manually
  rmse_train <- sqrt(mean((datatrainhealth[[response_col]] - train_predictions)^2))
  rmse_test <- sqrt(mean((datatesthealth[[response_col]] - test_predictions)^2))

  # Storing the summary of the model
  results[[response_col]] <- c(rmse_train,rmse_test)
}
print(results)
```
PCR
```{r}
library(pls)
# Assuming 'data' is your DataFrame
resultspcr <- list()

for (response_col in names(datahealth)) {
  # Selecting the current column as response and the rest as predictors
  datatraingrocery[,30] <- datatrainhealth[[response_col]]

  # Running the linear regression
  pcr_model <- pcr(V30 ~ ., data = datatraingrocery, scale = TRUE, validation = "CV")
  
  datatraingrocery <- datatraingrocery[,-30]

  # Predict on the training data
  train_predictions <- predict(pcr_model, newdata = datatraingrocery)

  # Predict on the testing data
  test_predictions <- predict(pcr_model, newdata = datatestgrocery)
    
  # Calculate RMSE manually
  rmse_train <- sqrt(mean((datatrainhealth[[response_col]] - train_predictions)^2))
  rmse_test <- sqrt(mean((datatesthealth[[response_col]] - test_predictions)^2))
  
  # Storing the summary of the model
  resultspcr[[response_col]] <- c(rmse_train,rmse_test)
}

print(resultspcr)
```
PLS
```{r}
# Assuming 'data' is your DataFrame
resultspls <- list()

for (response_col in names(datahealth)) {
  # Selecting the current column as response and the rest as predictors
  datatraingrocery[,30] <- datatrainhealth[[response_col]]

  # Running the linear regression
  pls_model <- plsr(V30 ~ ., data = datatraingrocery, scale = TRUE, validation = "CV")

  datatraingrocery <- datatraingrocery[,-30]

  # Predict on the training data
  train_predictions <- predict(pls_model, newdata = datatraingrocery)

  # Predict on the testing data
  test_predictions <- predict(pls_model, newdata = datatestgrocery)
    
  # Calculate RMSE manually
  rmse_train <- sqrt(mean((datatrainhealth[[response_col]] - train_predictions)^2))
  rmse_test <- sqrt(mean((datatesthealth[[response_col]] - test_predictions)^2))
  
  # Storing the summary of the model
  resultspls[[response_col]] <- c(rmse_train,rmse_test)
}
print(resultspls)

```
Ridge

```{r}
library(glmnet)

# Assuming 'data' is your DataFrame
resultsridge <- list()

for (response_col in names(datahealth)) {
  # Assuming predictors and response are matrices/vectors
  x <- as.matrix(datatraingrocery)
  y <- as.vector(datatrainhealth[[response_col]])

  # Find the best lambda using cross-validation
  cv_ridge <- cv.glmnet(x, y, alpha = 0)

  # Ridge model at optimal lambda
  ridge_model <- glmnet(x, y, alpha = 0, lambda = cv_ridge$lambda.min)

  # Predict on the training data
  train_predictions <- predict(ridge_model, newx = as.matrix(datatraingrocery))

  # Predict on the testing data
  test_predictions <- predict(ridge_model, newx = as.matrix(datatestgrocery))
    
  # Calculate RMSE manually
  rmse_train <- sqrt(mean((datatrainhealth[[response_col]] - train_predictions)^2))
  rmse_test <- sqrt(mean((datatesthealth[[response_col]] - test_predictions)^2))
  
  # Storing the summary of the model
  resultsridge[[response_col]] <- c(rmse_train,rmse_test)
}

print(resultsridge)
```
Lasso

```{r}
# Assuming 'data' is your DataFrame
resultslasso <- list()

for (response_col in names(datahealth)) {
  # Assuming predictors and response are matrices/vectors
  x <- as.matrix(datatraingrocery)
  y <- as.vector(datatrainhealth[[response_col]])

  # Find the best lambda using cross-validation
  cv_ridge <- cv.glmnet(x, y, alpha = 1)

  # Ridge model at optimal lambda
  ridge_model <- glmnet(x, y, alpha = 1, lambda = cv_ridge$lambda.min)

  # Predict on the training data
  train_predictions <- predict(ridge_model, newx = as.matrix(datatraingrocery))

  # Predict on the testing data
  test_predictions <- predict(ridge_model, newx = as.matrix(datatestgrocery))
    
  # Calculate RMSE manually
  rmse_train <- sqrt(mean((datatrainhealth[[response_col]] - train_predictions)^2))
  rmse_test <- sqrt(mean((datatesthealth[[response_col]] - test_predictions)^2))
  
  # Storing the summary of the model
  resultslasso[[response_col]] <- c(rmse_train,rmse_test)
}

print(resultslasso)
```